{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "import csv\n",
    "from datetime import datetime, timedelta\n",
    "import pytz\n",
    "import logging\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple example to fetch intraday stock data \n",
    "- 1 stock, 1 hour frequency\n",
    "- Try for 1 day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the URL\n",
    "url = \"https://data.alpaca.markets/v2/stocks/bars\"\n",
    "\n",
    "# Define the headers with your API key and secret\n",
    "headers = {\n",
    "    \"accept\": \"application/json\",\n",
    "    \"APCA-API-KEY-ID\": \"PKY8BZXWHLVDZ23RBZG9\",\n",
    "    \"APCA-API-SECRET-KEY\": \"dIDVua0J76AwcHPKb65ZhYVNFB441XhiRELXcoee\"\n",
    "}\n",
    "# I have to do for 15 stocks \n",
    "# March 15, 2019 to March 15, 2024\n",
    "# I cant put the whole start and end date because it will go over the page\n",
    "# And I'm not sure how to use the next page token\n",
    "# So better to divide the API query for every week? \n",
    "# Define the query parameters\n",
    "#AAPL,MSFT,NVDA,GOOGL,AMZN,JPM,V,MA,BAC,WFC,LLY,UNH,JNJ,MRK,ABBV\n",
    "params = {\n",
    "    \"symbols\": \"AAPL\",\n",
    "    \"timeframe\": \"1Hour\",\n",
    "    \"start\": \"2019-03-15T08:00:00Z\", #2019-03-15\n",
    "    \"end\": \"2019-03-16T21:45:00Z\", #2019-03-26\n",
    "    \"limit\": 1000,\n",
    "    \"adjustment\": \"raw\",\n",
    "    \"feed\": \"sip\",\n",
    "    \"sort\": \"asc\"\n",
    "}\n",
    "\n",
    "# Make the request\n",
    "response = requests.get(url, headers=headers, params=params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the response\n",
    "data = response.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'bars': {'AAPL': [{'c': 184.7,\n",
       "    'h': 185.33,\n",
       "    'l': 184.25,\n",
       "    'n': 138,\n",
       "    'o': 184.25,\n",
       "    't': '2019-03-15T08:00:00Z',\n",
       "    'v': 20187,\n",
       "    'vw': 184.712259},\n",
       "   {'c': 184.7,\n",
       "    'h': 184.7,\n",
       "    'l': 184.5,\n",
       "    'n': 22,\n",
       "    'o': 184.69,\n",
       "    't': '2019-03-15T09:00:00Z',\n",
       "    'v': 5491,\n",
       "    'vw': 184.595864},\n",
       "   {'c': 185.14,\n",
       "    'h': 185.17,\n",
       "    'l': 184.67,\n",
       "    'n': 31,\n",
       "    'o': 184.7,\n",
       "    't': '2019-03-15T10:00:00Z',\n",
       "    'v': 4924,\n",
       "    'vw': 184.891137},\n",
       "   {'c': 184.9,\n",
       "    'h': 185.12,\n",
       "    'l': 184.72,\n",
       "    'n': 304,\n",
       "    'o': 185.12,\n",
       "    't': '2019-03-15T11:00:00Z',\n",
       "    'v': 46489,\n",
       "    'vw': 184.88643},\n",
       "   {'c': 184.65,\n",
       "    'h': 185.25,\n",
       "    'l': 184.2,\n",
       "    'n': 667,\n",
       "    'o': 184.9982,\n",
       "    't': '2019-03-15T12:00:00Z',\n",
       "    'v': 115819,\n",
       "    'vw': 184.921543},\n",
       "   {'c': 184.91,\n",
       "    'h': 184.99,\n",
       "    'l': 183.74,\n",
       "    'n': 27525,\n",
       "    'o': 184.65,\n",
       "    't': '2019-03-15T13:00:00Z',\n",
       "    'v': 15247855,\n",
       "    'vw': 184.622},\n",
       "   {'c': 185.64,\n",
       "    'h': 185.67,\n",
       "    'l': 184.43,\n",
       "    'n': 33141,\n",
       "    'o': 184.91,\n",
       "    't': '2019-03-15T14:00:00Z',\n",
       "    'v': 4475954,\n",
       "    'vw': 185.056305},\n",
       "   {'c': 186.41,\n",
       "    'h': 186.51,\n",
       "    'l': 185.52,\n",
       "    'n': 30146,\n",
       "    'o': 185.65,\n",
       "    't': '2019-03-15T15:00:00Z',\n",
       "    'v': 4283026,\n",
       "    'vw': 186.130185},\n",
       "   {'c': 186.445,\n",
       "    'h': 186.98,\n",
       "    'l': 186.28,\n",
       "    'n': 24003,\n",
       "    'o': 186.41,\n",
       "    't': '2019-03-15T16:00:00Z',\n",
       "    'v': 3359598,\n",
       "    'vw': 186.677022},\n",
       "   {'c': 187.18,\n",
       "    'h': 187.25,\n",
       "    'l': 186.43,\n",
       "    'n': 18410,\n",
       "    'o': 186.46,\n",
       "    't': '2019-03-15T17:00:00Z',\n",
       "    'v': 2440892,\n",
       "    'vw': 186.841488},\n",
       "   {'c': 187.11,\n",
       "    'h': 187.21,\n",
       "    'l': 186.68,\n",
       "    'n': 21807,\n",
       "    'o': 187.18,\n",
       "    't': '2019-03-15T18:00:00Z',\n",
       "    'v': 2691148,\n",
       "    'vw': 187.034832},\n",
       "   {'c': 186.22,\n",
       "    'h': 187.33,\n",
       "    'l': 185.89,\n",
       "    'n': 44702,\n",
       "    'o': 187.11,\n",
       "    't': '2019-03-15T19:00:00Z',\n",
       "    'v': 6190829,\n",
       "    'vw': 186.55693},\n",
       "   {'c': 186.02,\n",
       "    'h': 186.4714,\n",
       "    'l': 185.97,\n",
       "    'n': 720,\n",
       "    'o': 186.12,\n",
       "    't': '2019-03-15T20:00:00Z',\n",
       "    'v': 10140420,\n",
       "    'vw': 186.119695},\n",
       "   {'c': 185.9,\n",
       "    'h': 186.03,\n",
       "    'l': 185.9,\n",
       "    'n': 141,\n",
       "    'o': 186.02,\n",
       "    't': '2019-03-15T21:00:00Z',\n",
       "    'v': 25427,\n",
       "    'vw': 186.007574}]},\n",
       " 'next_page_token': None}"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sample function to save file to csv "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_to_csv_test(stock, data):\n",
    "    filename = f\"{stock}_data.csv\"\n",
    "    with open(filename, 'w', newline='') as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerow(['timestamp', 'open', 'high', 'low', 'close', 'volume', 'numtrades', 'vwap'])\n",
    "        for entry in data['bars'][stock]:\n",
    "            writer.writerow([\n",
    "                entry['t'], entry['o'], entry['h'], entry['l'], entry['c'],\n",
    "                entry['v'], entry['n'], entry['vw']\n",
    "            ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_to_csv_test('AAPL', data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fetching intraday of 15Min interval for 15 stocks Mar'19 - Mar'24 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to generate weekly intervals \n",
    "# Want the start date to start at 08:00:00 UTC/04:00:00 EST \n",
    "# Want the end date to end at 21:45:00 UTC/17:45:00 EST\n",
    "\n",
    "def generate_intervals(start, end, interval_days=7):\n",
    "    intervals = []\n",
    "    current_start = start\n",
    "    while current_start < end:\n",
    "        week_end = current_start + timedelta(days=interval_days - 1, hours=13, minutes=45)\n",
    "        if week_end > end:\n",
    "            week_end = end\n",
    "        intervals.append((current_start, week_end))\n",
    "        current_start = week_end + timedelta(days=1, hours=-13, minutes=-45)\n",
    "    return intervals "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting up logging to ensure each week interval has a succesful API request\n",
    "log_filename = datetime.now().strftime('data_fetch_log_%Y%m%d_%H%M%S.log')\n",
    "\n",
    "logging.basicConfig(\n",
    "    filename=log_filename,\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fetch data from the API\n",
    "def fetch_data(stock, start, end):\n",
    "    url = \"https://data.alpaca.markets/v2/stocks/bars\"\n",
    "\n",
    "    # Define the headers with your API key and secret\n",
    "    headers = {\n",
    "        \"accept\": \"application/json\",\n",
    "        \"APCA-API-KEY-ID\": \"PKY8BZXWHLVDZ23RBZG9\",\n",
    "        \"APCA-API-SECRET-KEY\": \"dIDVua0J76AwcHPKb65ZhYVNFB441XhiRELXcoee\"\n",
    "    }\n",
    "\n",
    "    start_str = start.isoformat().replace(\"+00:00\", \"Z\")\n",
    "    end_str = end.isoformat().replace(\"+00:00\", \"Z\")\n",
    "\n",
    "    params = {\n",
    "        \"symbols\": stock,\n",
    "        \"timeframe\": \"15Min\",\n",
    "        \"start\": start_str,\n",
    "        \"end\": end_str,\n",
    "        \"limit\": 1000,\n",
    "        \"adjustment\": \"raw\",\n",
    "        \"feed\": \"sip\",\n",
    "        \"sort\": \"asc\"\n",
    "    }\n",
    "\n",
    "    for attempt in range(5):\n",
    "        try:\n",
    "            response = requests.get(url, headers=headers, params=params)\n",
    "            response.raise_for_status()\n",
    "            logging.info(f\"Data fetched successfully for {stock} from {start} to {end}.\")\n",
    "            return response.json()\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            if response.status_code == 429: # For too many requests\n",
    "                logging.warning(f\"Rate limit exceeded, retrying... (Attempt {attempt + 1})\")\n",
    "                time.sleep(60)\n",
    "            else:\n",
    "                logging.error(f\"Error fetching data for {stock} from {start} to {end}: {e}\")\n",
    "                return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_to_csv(stock, data, interval_num, start, end):\n",
    "    start_str = start.strftime('%Y%m%d_%H%M')\n",
    "    end_str = end.strftime('%Y%m%d_%H%M')\n",
    "    filename = f\"{stock}_data_interval_{interval_num}_{start_str}_to_{end_str}.csv\"\n",
    "    #filename = f\"{stock}_data_{interval_num}.csv\"\n",
    "    try:\n",
    "        with open(filename, 'w', newline='') as file:\n",
    "            writer = csv.writer(file)\n",
    "            writer.writerow(['timestamp', 'open', 'high', 'low', 'close', 'volume', 'numtrades', 'vwap'])\n",
    "            for entry in data['bars'][stock]:\n",
    "                writer.writerow([\n",
    "                    entry['t'], entry['o'], entry['h'], entry['l'], entry['c'],\n",
    "                    entry['v'], entry['n'], entry['vw']\n",
    "                ])\n",
    "        logging.info(f\"Data successfully saved to {filename}.\")\n",
    "    except IOError as e:\n",
    "        logging.error(f\"Error saving data to {filename}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test Run\n",
    "# stocks_test = ['AAPL', 'MSFT']\n",
    "# START_DATE_test = datetime(2019, 3, 15, 8, 0, 0, tzinfo=pytz.UTC)\n",
    "# END_DATE_test = datetime(2019, 4, 15, 21, 45, 0, tzinfo=pytz.UTC)\n",
    "\n",
    "# Setting up the dates for API request \n",
    "START_DATE = datetime(2019, 3, 15, 8, 0, 0, tzinfo=pytz.UTC)\n",
    "END_DATE = datetime(2024, 3, 15, 21, 45, 0, tzinfo=pytz.UTC)\n",
    "\n",
    "stocks = ['AAPL', 'MSFT', 'NVDA', 'GOOGL', 'AMZN', 'JPM', 'V', 'MA', 'BAC', 'WFC', 'LLY', 'UNH', 'JNJ', 'MRK', 'ABBV']\n",
    "weekly_intervals = generate_intervals(START_DATE, END_DATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking how many intervals I have \n",
    "for i, (start, end) in enumerate(weekly_intervals, start=1):\n",
    "    start_str = start.strftime('%Y%m%d_%H%M')\n",
    "    end_str = end.strftime('%Y%m%d_%H%M')\n",
    "    #print(i, start_str, end_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Another test just to make sure all of the variables are correct and we can run a simple fetch using API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2019-03-21T21:45:00Z'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "weekly_intervals[0][0].isoformat().replace(\"+00:00\", \"Z\")\n",
    "weekly_intervals[0][1].isoformat().replace(\"+00:00\", \"Z\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "stock_data = fetch_data(\"AAPL\", weekly_intervals[0][0], weekly_intervals[0][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "#stock_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Actual Run "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing AAPL...\n",
      "Processing MSFT...\n",
      "Processing NVDA...\n",
      "Processing GOOGL...\n",
      "Processing AMZN...\n",
      "Processing JPM...\n",
      "Processing V...\n",
      "Processing MA...\n",
      "Processing BAC...\n",
      "Processing WFC...\n",
      "Processing LLY...\n",
      "Processing UNH...\n",
      "Processing JNJ...\n",
      "Processing MRK...\n",
      "Processing ABBV...\n"
     ]
    }
   ],
   "source": [
    "for stock in stocks: \n",
    "    print(f\"Processing {stock}...\")\n",
    "    for i, (start, end) in enumerate(weekly_intervals, start=1):\n",
    "        stock_data = fetch_data(stock, start, end)\n",
    "        if 'bars' in stock_data and stock in stock_data['bars']:\n",
    "            save_to_csv(stock, stock_data, i, start, end)\n",
    "        else:\n",
    "            logging.warning(f\"No data for {stock} from {start} to {end}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Concatenate all csv files per stock"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_dir = 'Interval_StockData'\n",
    "stocks = ['AAPL', 'MSFT', 'NVDA', 'GOOGL', 'AMZN', 'JPM', 'V', 'MA', 'BAC', 'WFC', 'LLY', 'UNH', 'JNJ', 'MRK', 'ABBV']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def concatenate_csvs(stock):\n",
    "    pattern = os.path.join(csv_dir, f\"{stock}_data_interval_*.csv\")\n",
    "    all_files = glob.glob(pattern)\n",
    "    all_files.sort(key=lambda x: int(os.path.basename(x).split('_')[3]))\n",
    "    \n",
    "    df_list = []\n",
    "    for filename in all_files:\n",
    "        df = pd.read_csv(filename)\n",
    "        df_list.append(df)\n",
    "    combined_df = pd.concat(df_list, ignore_index=True)\n",
    "\n",
    "    combined_df.to_csv(f\"{stock}_intraday.csv\", index=False)\n",
    "    print(f\"Finished merging {stock}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished merging AAPL\n",
      "Finished merging MSFT\n",
      "Finished merging NVDA\n",
      "Finished merging GOOGL\n",
      "Finished merging AMZN\n",
      "Finished merging JPM\n",
      "Finished merging V\n",
      "Finished merging MA\n",
      "Finished merging BAC\n",
      "Finished merging WFC\n",
      "Finished merging LLY\n",
      "Finished merging UNH\n",
      "Finished merging JNJ\n",
      "Finished merging MRK\n",
      "Finished merging ABBV\n"
     ]
    }
   ],
   "source": [
    "for stock in stocks:\n",
    "    concatenate_csvs(stock)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
